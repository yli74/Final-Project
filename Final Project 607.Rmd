---
title: "Final Project: Netflix Orginals reviews with Sentiment Analysis"
output: html_document
---

#Overview and motivation

After conducting the research on Netflix's recommender system a few weeks ago, I was intrigued by how they used the the algorithm and predicted the rise of Netflix Originals such as House of Cards. From the perspective of a marketer, I have two main motivations to conduct the sentimental analysis because it provides a in depth look into consumer's attitudes towards the product (House of Cards) and whether it will contimue lead the trend.

To determine the quality of a movie, I'll conduct Twitter Sentimental Analysis then compare with the usuer's reviews on IMDb (Internet Movie Databse) since it has over 45 million registered users and serves as the biggest online movie databses. 

#Data Science Workflow

Step 1: Search Twitter for "" and collect tweet text, scrape IMDb for user reviews

Step 2: Load sentiment word lists and Get the scored sentiment for each source

Step 3: Clean, transform and analyze the data sets

Step 4: Compare Twitter sentiment with Datumbox API key

Step 5: Compare Twitter sentiment analysis with IMDb ratings

Step 6: Conclusion

#Data sources 

For this project, the following data sources were used via scarping web pages, web API's and downloads of CSV files. 

1. Twitter APIs

<https://dev.twitter.com/overview/api>

2. DatumBox

<https://www.datumbox.com/apikeys/>

3. Web Scraping 

<http://www.imdb.com/title/tt1856010/reviews?filter=best;filter=best;start=>
<http://www.imdb.com/title/tt1856010/reviews?filter=prolific;filter=prolific;start=>

4. CSV files and MySQL

For reference, I downloaded two files "positive words. txt" and " negative words. txt" from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html ("Opinion Mining, Sentiment Analysis, and Opinion Spam Detection"). I alos write .csv file and dump in MySQL

#Data Analysis

a.One data transfomrtaion operation (wide to long, columns to data format) 

  I used "dplyr" and "tidyr" to reshape and clean up the my raw data and dump into MySQL

b.One statistical Analysis and one graphic to describe or validate my data and conclusion(s)

  Twitter Sentimental Analysis : NLP, Datumbox APIs, Sentiment,ThinkToStartR, WordCloud, ggplot2
  IMDb User Reviews : Sentiment, Naive Bayes Classifiers,  SVM, MAXENT, Predictive Analysis, ggplot2
  
c.One feature that we did not cover in class

  I work with Twitter APIs and Datumbox APIs, and perfrom sentimental analysis on IMDb user reviews.

#Twitter Sentimental Analysis part I: Data Acquistion and Transformation

  a. web scapring user reviews (best reviews and prolific filters)
  b. save the data as .csv file and import it to MySQL
  c. perform visulization analysis and document classifcation with naive bayers

```{r}
library(twitteR)
library(plyr)
library(stringr)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr )
library(tidyr)
library(ThinkToStartR)
library(plyr)
library(RCurl)
library(wordcloud)

#set up twitter APIs
consumer_key <- "yGfMZdr02SidVYVTiul5YdJIh"
consumer_secret <- "ZxTcNmfJQ2cxLlykvYbZFFn0upknNVkpmdxAiGRDCZwIFN8wkH"
token <- "3192371979-CPwxqkws80IXLReSe1yMYl9QWEzCSEIv37lnPlZ" 
token_secret <- "ZFeXPlFmA8hsDs6dCAeR4E909lIkfcSjUFGLQ3jGlmwo4" #Access token secret
setup_twitter_oauth(consumer_key, consumer_secret, token, token_secret)

#Get tweets about "House of Cards", due to the limitation, we'll set n=3000
netflix.tweets<- searchTwitter("#HouseofCards",n=3000,lang="en") 
tweet=netflix.tweets[[1]]
tweet$getScreenName()
tweet$getText()
netflix.text=laply(netflix.tweets,function(t)t$getText())
length(netflix.text)
head(netflix.text) 

#performing data cleaning and store in csv.file
netflix.text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", netflix.text)
netflix.text = gsub("@\\w+", "", netflix.text)
netflix.text = gsub("[[:punct:]]", "", netflix.text)
netflix.text = gsub("[[:digit:]]", "", netflix.text)
netflix.text = gsub("http\\w+", "", netflix.text)
netflix.text = gsub("[ \t]{2,}", "", netflix.text)
netflix.text = gsub("^\\s+|\\s+$", "", netflix.text)
netflix.text <- str_replace_all(netflix.text," "," ")
netflix.text <- str_replace_all(netflix.text,"#[a-z,A-Z]*","")
netflix.text <- str_replace_all(netflix.text,"@[a-z,A-Z]*","")  
netflix.text <- tolower(netflix.text)
head(netflix.text)
write(netflix.text, "HouseofCards_Tweets.csv",ncolumn=1)

#perform sentimental analysis 
sentiment <- get_nrc_sentiment(netflix.text)
head(sentiment)

#perform data transformation and add the sentimental analysis into tweets
netflix.text <- cbind(netflix.text, sentiment)
head(netflix.text)
```

#Twitter Sentimental Analysis part II: Data visualzation

a. Use ggplot2 to visualize the finding

b. Datumbox offers more detailed sentiment analysis for tweets, and thanks to Julian Hillebrand, he has created a new package which contained the sentiment analyaiss on Twitter with Datumbox Key and wordcloud. The result shows that most of the tweets are neutral which matches with the sentimental analysis based on the positive and negative.txt from Hu and Liu. 

```{r}
#use the graphic to present the findings

#set up y = count
Totals<- data.frame(colSums(netflix.text[,c(11:10)]))
names(Totals) <- "score"

#set up x = sentiment
Totals <- cbind("sentiment" = rownames(Totals), Totals)
rownames(Totals) <- NULL

ggplot(Totals, aes(x = sentiment, y = score)) +
        geom_bar(aes(fill = sentiment), stat = "identity", position = "dodge", width = 1) +
        xlab("sentiment") + ylab("sentiment Scores") + ggtitle("Sentiment Scores for All Tweets")

##Sentimental Analysis with Datumbox API
#set up twitter APIs
consumer_key <- "yGfMZdr02SidVYVTiul5YdJIh"
consumer_secret <- "ZxTcNmfJQ2cxLlykvYbZFFn0upknNVkpmdxAiGRDCZwIFN8wkH"
token <- "3192371979-CPwxqkws80IXLReSe1yMYl9QWEzCSEIv37lnPlZ" 
token_secret <- "ZFeXPlFmA8hsDs6dCAeR4E909lIkfcSjUFGLQ3jGlmwo4" #Access token secret
setup_twitter_oauth(consumer_key, consumer_secret, token, token_secret)

ThinkToStart("setup_twitter_oauth",api_key="yGfMZdr02SidVYVTiul5YdJIh",api_secret="ZxTcNmfJQ2cxLlykvYbZFFn0upknNVkpmdxAiGRDCZwIFN8wkH")

ThinkToStart("SentimentCloud","#HouseofCards",30,"2c2aab873c34498ba6624bfdf0cf2faf")
```

#IMDb User Reveiew Analysis

  a. web scapring user reviews (best reviews and prolific filters)
  b. write the result as .csv file and export to MySQL
  c. perform visulization analysis and document classifcation with naive bayes classifiers

```{r}
library(twitteR)
library(stringr)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(tidyr)
library(plyr)
library(ThinkToStartR)
library(RCurl)
library(wordcloud)
library(rvest)
library(XML)
library(tm)
library(RTextTools)
library(e1071)

#retrieve top reviews from best reviews and prolific authors on IMDb.com
counts = c(0,10,20,30,40,50,60,70,80,90,100)
reviews = NULL
for (j in counts){
  
  page1 = read_html(paste0("http://www.imdb.com/title/tt1856010/reviews?filter=best;filter=best;start=",j))
  page2 = read_html(paste0("http://www.imdb.com/title/tt1856010/reviews?filter=prolific;filter=prolific;start=",j))
  
  reviews1 <- page1 %>% html_nodes("#tn15content") %>%
  html_nodes("div+p") %>%
  html_text()
  
  reviews2 <- page2 %>% html_nodes("#tn15content") %>%
  html_nodes("div+p") %>%
  html_text()
  
  best.reviews = setdiff(reviews1, c("*** This review may contain spoilers ***"))
  prolific.authors = setdiff(reviews2, c("*** This review may contain spoilers ***"))
  
  reviews <- c(reviews,best.reviews,prolific.authors)
  reviews <- gsub("\r?\n|\r", " ", reviews) 
  reviews <- tolower(gsub("[^[:alnum:] ]", " ", reviews))
  reviews <- gsub("<.*?>", " ", reviews)
  reviews <- iconv(reviews, "latin1", "ASCII", sub="")
  reviews <-  removeNumbers(reviews)  
  reviews <- stripWhitespace(reviews)
  reviews <-  gsub("^\\s+|\\s+$", "", reviews)
}

  IMDbsentiment <- cbind(get_nrc_sentiment(reviews), reviews)
  
  #Convert the list to a data frame then convert to a tbl_df format
  clean.reviews = data.frame(text = reviews,class = get_nrc_sentiment(reviews), stringsAsFactors = T)
  str(clean.reviews)
  clean = as.data.frame(clean.reviews)
  head(clean)
  clean_df <- tbl_df(clean)  
  write.csv(clean.reviews, "IMDb.CSV")
  
  #Perfrom data cleaning and calculate the final score 
  IMDb1<- clean_df %>%
   select(text, positive=class.positive, negative= class.negative, -(class.anger:class.trust))%>%
   mutate(Total = positive - negative)
  head(IMDb1)
  
  write.csv(IMDb1, "TotalIMDb.CSV")
  
  FinalIMDb <- select(IMDb1,text,Total)
  head(FinalIMDb)
  
  #reshape my data then visualize the analyis
  IMDb1  %>% gather(sentiment, scores, -text) %>%
    ggplot(data = .) + 
    geom_line(aes(x = text, y = scores, group = sentiment, color = sentiment))+ggtitle("Compare Sentimental Scores for IMDb User Reviews")

 clean_df   %>% 
    gather(sentiment, scores, -text) %>%
    ggplot() +
    geom_bar(aes(y = scores, x = text, fill = sentiment), stat = "identity")+ggtitle("Total Sentimental Scores for IMDb User Reviews")

#import the data frame into MySQL
library("RMySQL")
library("knitr")
dbConnection<- dbConnect(dbDriver("MySQL"), user="root", password="", dbname="IMDb", host="127.0.0.1", port=3306)
dbWriteTable(conn = dbConnection, name = 'IMDb88', value = as.data.frame(FinalIMDb))

all_cons <- dbListConnections(MySQL())
for(con in all_cons) 
  dbDisconnect(con) 

#document classification

doc_matrix <- create_matrix(FinalIMDb$text, language="english", removeNumbers=TRUE,
stemWords=TRUE, removeSparseTerms=.99)
doc_matrix

container <- create_container(doc_matrix, FinalIMDb$text, trainSize=1:109,
testSize=110:122, virgin=FALSE)

SVM <- train_model(container,"SVM")
MAXENT <- train_model(container,"MAXENT")

SVM_CLASSIFY <- classify_model(container, SVM)
MAXENT_CLASSIFY <- classify_model(container, MAXENT)

head(SVM_CLASSIFY)
head(MAXENT_CLASSIFY)

```

#Conclusion and Challenges

In this projec, I used Twitter APIs, Datumbox APIs and web scraping from IMDb.com, and performed a variety of analysis on the datasets. As expected, House of Cards has received all positive reviews from the users on Twitter and IMDb, it was interesting to see how IMDb users had a relatively high degree of trust towards the show. I also realized that it was quite challenging to conduct sentimental analysis for the show because a lot of users like use so-called negative words to express their positive impressions, words like "House of Cards is killing it!" or "OMG no way." 

#References

[1] <http://diggdata.in/post/50938118301/fetching-twitter-data-in-r-with-oauth-handshake-feature>

[2] <https://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/>

[3] <http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html>

[4] <https://rpubs.com/michael-jules/visualizing-machine-learning>

[5] <https://www.r-bloggers.com/twitter-sentiment-analysis-with-r/>

[6] <http://thinktostart.com/sentiment-analysis-on-twitter-with-datumbox-api/#comments>



